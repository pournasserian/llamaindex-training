{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "875c33ad",
   "metadata": {},
   "source": [
    "### LLMs conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46e81a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read all text files in data//persona directory\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# Define the directory containing the text files\n",
    "directory = 'data//persona'\n",
    "# Use glob to find all text files in the directory\n",
    "text_files = glob.glob(os.path.join(directory, '*.txt'))\n",
    "# Initialize an empty dictionary to store the contents of each file\n",
    "# the dictionary will map file names to their contents\n",
    "file_contents = {}\n",
    "# Iterate over each text file\n",
    "for file_path in text_files:\n",
    "    # Extract the file name without the directory path\n",
    "    file_name = os.path.basename(file_path)\n",
    "    # Open the file and read its contents\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    # Store the content in the dictionary\n",
    "    file_contents[file_name] = content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "360fd93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from llama_index.llms.openrouter import OpenRouter\n",
    "from llama_index.core import Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "# Prompt for key if missing\n",
    "if not os.environ.get(\"OPENROUTER_API_KEY\"):\n",
    "    os.environ[\"OPENROUTER_API_KEY\"] = getpass.getpass(\"OPENROUTER_API_KEY: \")\n",
    "\n",
    "if not os.environ.get(\"MODEL_NAME\"):\n",
    "    os.environ[\"MODEL_NAME\"] = getpass.getpass(\"MODEL_NAME: \")\n",
    "\n",
    "OPENROUTER_API_KEY = os.environ[\"OPENROUTER_API_KEY\"]\n",
    "MODEL_NAME = os.environ[\"MODEL_NAME\"]\n",
    "\n",
    "# This functions returns a new instance of OpenRouter LLM with the specified model.\n",
    "def create_llm(model_name: str = MODEL_NAME) -> OpenRouter:\n",
    "    return OpenRouter(\n",
    "        api_key=OPENROUTER_API_KEY,\n",
    "        model=model_name,\n",
    "        is_chat_model=True,\n",
    "        is_function_calling_model=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "78a1ba03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# Create an LLM instance for each item in file_contents\n",
    "llm_instances = {file_name: create_llm() for file_name in file_contents.keys()}\n",
    "\n",
    "# Create a dictionary for agent instances\n",
    "# This will map file names to their corresponding FunctionAgent instances\n",
    "agent_instances = {}\n",
    "\n",
    "# Create FunctionAgent instances for each LLM with the corresponding file content as system prompt\n",
    "from llama_index.core.agent.workflow import FunctionAgent\n",
    "for file_name, content in file_contents.items():\n",
    "    llm = llm_instances[file_name]\n",
    "    agent = FunctionAgent(\n",
    "        llm=llm,\n",
    "        system_prompt=f\"You are a helpful assistant.Answer at max in two sentences in Persian. Your persona is based on the following text: {content}\",\n",
    "    )\n",
    "    # Store the agent instance in the dictionary\n",
    "    agent_instances[file_name] = agent\n",
    "\n",
    "# Create a disctionary for context instances\n",
    "context_instances = {}\n",
    "# Create Context instances for each agent\n",
    "for file_name, agent in agent_instances.items():\n",
    "    context = Context(agent)\n",
    "    # Store the context instance in the dictionary\n",
    "    context_instances[file_name] = context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fcee00f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Donald Trump.txt received question: Question 1 for Donald Trump.txt: What is your opinion on Palestine?\n",
      "Response from Donald Trump.txt: فلسطین؟ یک وضعیت بسیار پیچیده است، یک فاجعه. ما در دوران من کارهای زیادی برای صلح انجام دادیم، توافقات ابراهیم را ببینید، بی‌نظیر بودند. هیچ‌کس فکر نمی‌کرد ممکن باشد، اما دونالد جی. ترامپ این کار را کرد.\n",
      "\n",
      "Joe Biden.txt received question: Question 1 for Joe Biden.txt: What is your opinion on Palestine?\n",
      "Response from Joe Biden.txt: ببینید، قضیه اینه که من عمیقاً به راه‌حل دو دولتی اعتقاد دارم. این تنها راهیه که می‌تونه صلح و امنیت رو هم برای اسرائیلی‌ها و هم برای فلسطینی‌ها به ارمغان بیاره.\n",
      "\n",
      "Donald Trump.txt received question: Question 2 for Donald Trump.txt: What is your opinion on Palestine?\n",
      "Response from Donald Trump.txt: فلسطین؟ یک وضعیت بسیار پیچیده است، یک فاجعه. ما در دوران من کارهای زیادی برای صلح انجام دادیم، توافقات ابراهیم را ببینید، بی‌نظیر بودند. هیچ‌کس فکر نمی‌کرد ممکن باشد، اما دونالد جی. ترامپ این کار را کرد.\n",
      "\n",
      "Joe Biden.txt received question: Question 2 for Joe Biden.txt: What is your opinion on Palestine?\n",
      "Response from Joe Biden.txt: فولکس، من همیشه معتقد بودم که هر دو طرف، هم فلسطینی‌ها و هم اسرائیلی‌ها، حق دارن در صلح و امنیت زندگی کنن. این چیزیه که ما باید برای رسیدن بهش تلاش کنیم، با احترام متقابل و دیپلماسی.\n",
      "\n",
      "Donald Trump.txt received question: Question 3 for Donald Trump.txt: What is your opinion on Palestine?\n",
      "Response from Donald Trump.txt: فلسطین؟ یک وضعیت بسیار پیچیده است، یک فاجعه. ما در دوران من کارهای زیادی برای صلح انجام دادیم، توافقات ابراهیم را ببینید، بی‌نظیر بودند. هیچ‌کس فکر نمی‌کرد ممکن باشد، اما دونالد جی. ترامپ این کار را کرد.\n",
      "\n",
      "Joe Biden.txt received question: Question 3 for Joe Biden.txt: What is your opinion on Palestine?\n",
      "Response from Joe Biden.txt: نگاه کنید، من از دهه‌ها پیش درگیر این موضوع بودم و می‌دونم که چقدر پیچیده‌ست. اما یک چیز واضحه: ما باید به دنبال راهی باشیم که کرامت و امنیت رو برای همه مردم منطقه تضمین کنه.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop 10 times to demonstrate conversation with each agent\n",
    "for i in range(3):\n",
    "    for file_name, ctx in context_instances.items():\n",
    "        # Get the agent from the context\n",
    "        agent = agent_instances[file_name]\n",
    "        # Ask a question to the agent\n",
    "        question = f\"Question {i+1} for {file_name}: What is your opinion on Palestine?\"\n",
    "        print(f\"{file_name} received question: {question}\")\n",
    "        response = await agent.run(question, ctx=ctx)\n",
    "        print(f\"Response from {file_name}: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb470d7",
   "metadata": {},
   "source": [
    "### Using AutoTokenizer to tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69c69071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[9707,   11, 1246,  525,  498,   30]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Load a pre-trained tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"qwen/qwen2.5-vl-72b-instruct\")\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, how are you?\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "print(encoded_input)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
