{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec843d6c",
   "metadata": {},
   "source": [
    "### Basic Agent Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a27ea275",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import llm\n",
    "from llama_index.core.agent.workflow import FunctionAgent, AgentWorkflow\n",
    "\n",
    "# Define a simple calculator tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Useful for multiplying two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "\n",
    "# Create an agent workflow with our calculator tool\n",
    "agent = FunctionAgent(\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can multiply two numbers.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e710bb1",
   "metadata": {},
   "source": [
    "### Adding Chat History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1a22b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Logan! How can I help you today?\n",
      "\n",
      "Your name is Logan.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Context\n",
    "\n",
    "# create context\n",
    "ctx = Context(agent)\n",
    "\n",
    "# run agent with context\n",
    "response = await agent.run(\"My name is Logan\", ctx=ctx)\n",
    "print(str(response))\n",
    "\n",
    "response = await agent.run(\"What is my name?\", ctx=ctx)\n",
    "print(str(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b87841a",
   "metadata": {},
   "source": [
    "### Maintaining state over longer periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "948c46bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your name is Logan.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import JsonSerializer\n",
    "\n",
    "ctx_dict = ctx.to_dict(serializer=JsonSerializer())\n",
    "\n",
    "restored_ctx = Context.from_dict(\n",
    "    agent, ctx_dict, serializer=JsonSerializer()\n",
    ")\n",
    "\n",
    "response3 = await agent.run(user_msg=\"What's my name?\", ctx=restored_ctx)\n",
    "print(str(response3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73937bb0",
   "metadata": {},
   "source": [
    "### Tools and state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc82798f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What would you like to set your name to?\n",
      "Current state:\n",
      "{'name': 'Laurie'}\n",
      "\n",
      "Current message:\n",
      "Hello Laurie!\n",
      "Name as stored in state:  Laurie\n"
     ]
    }
   ],
   "source": [
    "async def set_name(ctx: Context, name: str) -> str:\n",
    "    async with ctx.store.edit_state() as ctx_state:\n",
    "        ctx_state[\"state\"][\"name\"] = name\n",
    "\n",
    "    return f\"Name set to {name}\"\n",
    "\n",
    "workflow = AgentWorkflow.from_tools_or_functions(\n",
    "    [set_name],\n",
    "    llm=llm,\n",
    "    system_prompt=\"You are a helpful assistant that can set a name.\",\n",
    "    initial_state={\"name\": \"unset\"},\n",
    ")\n",
    "\n",
    "ctx = Context(workflow)\n",
    "\n",
    "# check if it knows a name before setting it\n",
    "response = await workflow.run(user_msg=\"What's my name?\", ctx=ctx)\n",
    "print(str(response))\n",
    "\n",
    "response2 = await workflow.run(user_msg=\"My name is Laurie\", ctx=ctx)\n",
    "print(str(response2))\n",
    "\n",
    "state = await ctx.store.get(\"state\")\n",
    "print(\"Name as stored in state: \", state[\"name\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2541224",
   "metadata": {},
   "source": [
    "### Chat Engine - Condense Plus Context Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1b72206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paul Graham spent his childhood engaged in two main activities outside of school: writing and programming.\n",
      "\n",
      "He wrote short stories, which he describes as having \"hardly any plot, just characters with strong feelings.\"\n",
      "\n",
      "His early programming experiences began around age 13 or 14 on an IBM 1401 mainframe computer at his school district. He used Fortran and punch cards, though he found it difficult to do much with it due to limitations.\n",
      "\n",
      "Everything changed with microcomputers. He eventually convinced his father to buy him a TRS-80 around 1980, which is when he \"really started programming.\" He wrote simple games, a program to predict model rocket flight, and even a word processor for his father.\n",
      "----------------------------------------------------\n",
      "Certainly! Let's delve a bit deeper into Paul Graham's early life and experiences, based on the essay.\n",
      "\n",
      "**Early Programming Challenges and Discoveries:**\n",
      "\n",
      "*   **The IBM 1401:** His initial foray into programming on the IBM 1401 was quite limited. He couldn't figure out what to do with it because the only input was data on punch cards, which he didn't have. He also lacked the mathematical knowledge to create programs that didn't require input, like calculating pi approximations. His most vivid memory from this time is learning that programs could fail to terminate, a \"social as well as a technical error\" on a machine without time-sharing.\n",
      "*   **The Shift to Microcomputers:** The arrival of microcomputers was a game-changer. He was particularly impressed and envious when a friend built his own computer from a Heathkit. This personal access, where the computer could respond to keystrokes in real-time, was a significant improvement over the batch processing of punch cards.\n",
      "*   **The TRS-80 and Practical Applications:** Once he got his own TRS-80, his programming really took off. Beyond games, he created practical tools like the rocket prediction program and\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "\n",
    "data = SimpleDirectoryReader(input_dir=\"./data/\").load_data()\n",
    "index = VectorStoreIndex.from_documents(data)\n",
    "\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n",
    "\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"condense_plus_context\",\n",
    "    memory=memory,\n",
    "    llm=llm,\n",
    "    context_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "        \"Here are the relevant documents for the context:\\n\"\n",
    "        \"{context_str}\"\n",
    "        \"\\nInstruction: Use the previous chat history, or the context above, to interact and help the user.\"\n",
    "    ),\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"What did Paul Graham do growing up\")\n",
    "print(response)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "response_2 = chat_engine.chat(\"Can you tell me more?\")\n",
    "print(response_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25749813",
   "metadata": {},
   "source": [
    "### Reset conversation state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ba2a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know a lot of things! I'm a large language model, trained by Google. I can understand and generate human-like text, and I have access to a vast amount of information from the real world.\n",
      "\n",
      "What would you like to talk about? We could discuss the essay by Paul Graham, or anything else you have in mind!\n"
     ]
    }
   ],
   "source": [
    "chat_engine.reset()\n",
    "response = chat_engine.chat(\"Hello! What do you know?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f32f79b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: What did Paul Graham do after YC?\n",
      "After leaving YC, Paul Graham decided to paint, aiming to see how good he could become if he focused on it. He spent most of 2014 painting but eventually ran out of enthusiasm for it. Following this, he started writing essays again, including some not about startups. In March 2015, he began working on Lisp once more.\n",
      "----------------------------------------------------\n",
      "Querying with: What did Paul Graham do after he started writing essays again and working on Lisp in March 2015?\n",
      "The provided text does not contain information about what Paul Graham did in March 2015. It mentions that he wrote many essays and that O'Reilly reprinted a collection of them as a book called \"Hackers & Painters.\" He also worked on spam filters and did more painting.\n",
      "----------------------------------------------------\n",
      "Querying with: What did Paul Graham do after March 2015, specifically after he started working on Lisp again, writing essays, and doing more painting?\n",
      "The provided text does not contain information about Paul Graham's activities after March 2015. It mentions his work on Lisp, writing essays, and painting, but does not specify a timeline that extends beyond the publication of his collected essays as \"Hackers & Painters\" and his work on spam filters.\n",
      "----------------------------------------------------\n",
      "Querying with: What about after that?\n",
      "After that, the author started writing essays again, including some that were not about startups. In March 2015, the author began working on Lisp again.\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
    "response = chat_engine.chat(\"What did Paul Graham do after YC?\")\n",
    "print(response)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "response = chat_engine.chat(\"What about after that?\")\n",
    "print(response)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "response = chat_engine.chat(\"Can you tell me more?\")\n",
    "print(response)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "chat_engine.reset()\n",
    "response = chat_engine.chat(\"What about after that?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e202000",
   "metadata": {},
   "source": [
    "### Verbose mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc33353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying with: What did Paul Graham do after YC?\n",
      "After leaving YC, Paul Graham decided to take up painting, aiming to see how proficient he could become if he dedicated himself to it. He spent most of 2014 painting but eventually stopped when he lost interest. Following this, he resumed writing essays and, in March 2015, began working on Lisp again.\n",
      "----------------------------------------------------\n",
      "After leaving YC, Paul Graham decided to paint, aiming to see how good he could become if he focused on it. He spent most of 2014 painting but eventually ran out of steam and stopped. Following this, he started writing essays again, including some not about startups. In March 2015, he began working on Lisp once more.\n"
     ]
    }
   ],
   "source": [
    "chat_engine1 = index.as_chat_engine(chat_mode=\"condense_question\", verbose=True)\n",
    "response = chat_engine1.chat(\"What did Paul Graham do after YC?\")\n",
    "print(response)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "chat_engine2 = index.as_chat_engine(chat_mode=\"condense_question\", verbose=False)\n",
    "response = chat_engine2.chat(\"What did Paul Graham do after YC?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf00272",
   "metadata": {},
   "source": [
    "### Chat Engine - ReAct Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77b0fec0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ChatMode.REACT and ChatMode.OPENAI are now deprecated and removed. Please use the ReActAgent or FunctionAgent classes from llama_index.core.agent.workflow to create an agent with a query engine tool.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_engine\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatMode\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m chat_engine2 = \u001b[43mindex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_chat_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchat_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatMode\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREACT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# chat_engine_react = index.as_chat_engine(chat_mode=\"react\", llm=llm, verbose=True)\u001b[39;00m\n\u001b[32m      6\u001b[39m response = chat_engine2.chat(\n\u001b[32m      7\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mWhat are the first programs Paul Graham tried writing?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\amirp\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\llama_index\\core\\indices\\base.py:551\u001b[39m, in \u001b[36mBaseIndex.as_chat_engine\u001b[39m\u001b[34m(self, chat_mode, llm, **kwargs)\u001b[39m\n\u001b[32m    549\u001b[39m \u001b[38;5;66;03m# resolve chat mode\u001b[39;00m\n\u001b[32m    550\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_mode \u001b[38;5;129;01min\u001b[39;00m [ChatMode.REACT, ChatMode.OPENAI]:\n\u001b[32m--> \u001b[39m\u001b[32m551\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    552\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mChatMode.REACT and ChatMode.OPENAI are now deprecated and removed. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    553\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease use the ReActAgent or FunctionAgent classes from llama_index.core.agent.workflow \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    554\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mto create an agent with a query engine tool.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    555\u001b[39m     )\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chat_mode == ChatMode.CONDENSE_QUESTION:\n\u001b[32m    558\u001b[39m     \u001b[38;5;66;03m# NOTE: lazy import\u001b[39;00m\n\u001b[32m    559\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mllama_index\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mchat_engine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CondenseQuestionChatEngine\n",
      "\u001b[31mValueError\u001b[39m: ChatMode.REACT and ChatMode.OPENAI are now deprecated and removed. Please use the ReActAgent or FunctionAgent classes from llama_index.core.agent.workflow to create an agent with a query engine tool."
     ]
    }
   ],
   "source": [
    "from llama_index.core.chat_engine.types import ChatMode\n",
    "\n",
    "chat_engine2 = index.as_chat_engine(chat_mode=ChatMode.REACT, verbose=True)\n",
    "\n",
    "# chat_engine_react = index.as_chat_engine(chat_mode=\"react\", llm=llm, verbose=True)\n",
    "response = chat_engine2.chat(\n",
    "    \"What are the first programs Paul Graham tried writing?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8139b04e",
   "metadata": {},
   "source": [
    "### Chat Engine - Context Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27be57a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today? Are you interested in learning more about Paul Graham's essay, or perhaps something else?\n",
      "----------------------------------------------------\n",
      "Based on the essay, Paul Graham was involved in two main activities outside of school growing up:\n",
      "\n",
      "1.  **Writing:** He wrote short stories, though he describes them as \"awful\" with \"hardly any plot.\"\n",
      "2.  **Programming:** He started programming around 9th grade (age 13 or 14) on an IBM 1401 at his school district. Later, with the advent of microcomputers, he really got into programming after his father bought him a TRS-80 around 1980. He wrote simple games, a program to predict model rocket flight, and even a word processor for his father.\n",
      "----------------------------------------------------\n",
      "Certainly! Let's dive a bit deeper into Paul Graham's early life and experiences as described in the essay.\n",
      "\n",
      "**Early Programming Experiences:**\n",
      "\n",
      "*   **IBM 1401 (9th Grade):** His very first exposure to programming was on an IBM 1401. This was a mainframe computer, and his interaction was likely through punch cards. He mentions the \"thrill\" of seeing his program work, even if it was just printing a triangle of stars. This early experience clearly sparked something in him.\n",
      "*   **TRS-80 (around 1980):** This was a pivotal moment. The arrival of personal computers like the TRS-80 made programming much more accessible and immediate. He describes it as \"the first time I could program a computer that was actually there.\" This hands-on access allowed him to experiment freely.\n",
      "    *   **Games:** He wrote simple games, which is a common entry point for many young programmers, as it provides immediate visual feedback and a sense of accomplishment.\n",
      "    *   **Model Rocket Program:** This shows an early application of programming to a real-world interest, demonstrating a problem-solving mindset.\n",
      "    *   **Word Processor:** Writing a\n"
     ]
    }
   ],
   "source": [
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    memory=memory,\n",
    "    system_prompt=(\n",
    "        \"You are a chatbot, able to have normal interactions, as well as talk\"\n",
    "        \" about an essay discussing Paul Grahams life.\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "response = chat_engine.chat(\"Hello!\")\n",
    "print(response)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "response = chat_engine.chat(\"What did Paul Graham do growing up?\")\n",
    "print(response)\n",
    "\n",
    "print(\"----------------------------------------------------\")\n",
    "\n",
    "response = chat_engine.chat(\"Can you tell me more?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e4ad7b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know a lot about Paul Graham's life, based on his essay. For example, I know about his experiences working at Interleaf, his thoughts on art school, and his reflections on leaving Y Combinator.\n",
      "\n",
      "What are you interested in knowing about?\n"
     ]
    }
   ],
   "source": [
    "chat_engine.reset()\n",
    "response = chat_engine.chat(\"Hello! What do you know?\")\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
